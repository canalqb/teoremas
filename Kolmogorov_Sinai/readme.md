# **README - Estimativa de Entropia com base no Teorema de Kolmogorovâ€“Sinai**

âœ¨ **Bem-vindo a este projeto incrÃ­vel!** Aqui vocÃª vai aprender a estimar a entropia de uma sequÃªncia binÃ¡ria aleatÃ³ria usando o poderoso *Teorema de Kolmogorovâ€“Sinai* â€” uma ferramenta fundamental da teoria da informaÃ§Ã£o e ergÃ³dica. Vamos nessa? ğŸš€

---

## **SOBRE A TEORIA**

O *Teorema de Kolmogorovâ€“Sinai* (ou entropia KS) Ã© um conceito fascinante da *teoria ergÃ³dica* que conecta o comportamento caÃ³tico dos sistemas dinÃ¢micos com a geraÃ§Ã£o de informaÃ§Ã£o.

*Em termos simples:*
âœ¨ **A entropia mÃ©trica mede a quantidade mÃ¡xima de informaÃ§Ã£o (aleatoriedade) produzida por um sistema ao longo do tempo.**

Aplicando Ã  anÃ¡lise de sequÃªncias binÃ¡rias:

* Estimamos a entropia observando o crescimento da entropia em **blocos de tamanho k**.
* A taxa de entropia por bit Ã© dada por:

  *h = lim (k â†’ âˆ) Hâ‚– / k*

  Onde **Hâ‚–** Ã© a entropia dos blocos de k bits.

Se a sequÃªncia for realmente aleatÃ³ria, esperamos que *Hâ‚– / k* se aproxime de **1.0** â€” ou seja, **entropia mÃ¡xima por bit**! ğŸ‰

---

## **SOBRE O SCRIPT**

Este script Python (`entropia_ks.py`) Ã© um verdadeiro aliado para medir a aleatoriedade da sua fonte de bits!

Ele faz o seguinte:

1. Gera uma sequÃªncia binÃ¡ria longa e segura com o mÃ³dulo `secrets` ğŸ” â€” que produz nÃºmeros aleatÃ³rios criptograficamente seguros.
2. Para cada bloco de tamanho **k** (de 1 atÃ© um valor mÃ¡ximo que vocÃª escolher), divide a sequÃªncia em blocos.
3. Conta a frequÃªncia de cada bloco para calcular a **entropia de Shannon** (Hâ‚–).
4. Calcula a **entropia por bit** dividindo Hâ‚– por k.
5. Mostra uma tabela clara e organizada com os resultados â€” perfeita para analisar a qualidade da aleatoriedade! ğŸ“Š

---

## **EXEMPLO DE SAÃDA**

Veja como fica a saÃ­da do script para uma sequÃªncia idealmente aleatÃ³ria:

```
ğŸ“Š AnÃ¡lise de Entropia com base em Kolmogorovâ€“Sinai  
Total de bits gerados: 100000  
Fonte: secrets.token_bytes  

 k | Blocos Ãºnicos | Total blocos | H_k (bits) | H_k / k  
---|----------------|---------------|-------------|--------  
 1 |              2 |        100000 |     1.000000 | 1.0000  
 2 |              4 |         50000 |     2.000000 | 1.0000  
 3 |              8 |         33333 |     3.000000 | 1.0000  
 4 |             16 |         25000 |     4.000000 | 1.0000  
```

*Na prÃ¡tica*, quanto mais prÃ³ximo de **1.0** estiver *Hâ‚– / k*, mais aleatÃ³ria e imprevisÃ­vel Ã© sua fonte! ğŸ¥³ Caso contrÃ¡rio, pode haver padrÃµes ou redundÃ¢ncias.

---

## **O QUE VOCÃŠ VAI APRENDER COM ESTE PROJETO**

* Como aplicar a teoria ergÃ³dica para analisar dados reais ğŸ”
* A relaÃ§Ã£o entre **entropia empÃ­rica** e a qualidade da aleatoriedade
* Uso prÃ¡tico do Python para anÃ¡lise estatÃ­stica e teoria da informaÃ§Ã£o ğŸ
* Avaliar se seu gerador de nÃºmeros aleatÃ³rios Ã© confiÃ¡vel ou nÃ£o ğŸ’¡

---

## **REQUISITOS**

* Python 3.7 ou superior ğŸ
* Sem dependÃªncias externas â€” apenas mÃ³dulos padrÃ£o (`secrets`, `math`, `collections`)

---

## **COMO EXECUTAR**

1. Salve o arquivo `entropia_ks.py`.
2. No terminal, execute:

```
python entropia_ks.py
```

3. Quer testar com mais bits ou blocos maiores? Modifique o final do script:

```python
estimar_entropia_ks(n_bits=100_000, k_max=16)
```

---

## **ESTRUTURA DO PROJETO**

* `entropia_ks.py` â€” script principal de anÃ¡lise ğŸ”§
* `README.md` â€” esta documentaÃ§Ã£o ğŸ“š

---

## **REFERÃŠNCIAS INSPIRADORAS**

* A.N. Kolmogorov â€” Entropy per Unit Time as a Metric Invariant of Automorphisms
* C.E. Shannon â€” A Mathematical Theory of Communication
* Cover & Thomas â€” Elements of Information Theory

---

## **LICENÃ‡A**

Este projeto Ã© **open source** sob a licenÃ§a **MIT**. Sinta-se livre para usar, modificar e compartilhar! â¤ï¸

--- 
  
## ğŸ“¬ Contato

* Feito por CanalQb no GitHub 
* Visite o blog: canalqb.blogspot.com [https://canalqb.blogspot.com]
* ğŸ’¸ Apoie o projeto via Bitcoin: 13Ve1k5ivByaCQ5yer6GoV84wAtf3kNava
* PIX: qrodrigob@gmail.com
